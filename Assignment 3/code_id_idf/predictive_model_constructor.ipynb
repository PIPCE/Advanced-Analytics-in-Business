{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.203:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.203:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20ef8467248>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"tweet_id\": 1380150333767262211, \"tweet_text\": \"#\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588 bubbles under surface....\\\\n\\\\n#\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588 Backs #\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588, but Some Call for a Clearer Warning to #\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588 https://t.co/56tmzwLaS0\", \"label\": \"#biden\"}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile('file:///C:/Users/lenne/Desktop/spark/coding_and_data/data/lots_of_data/tweets')\n",
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+\n",
      "|   label|           tweet_id|          tweet_text|\n",
      "+--------+-------------------+--------------------+\n",
      "|  #biden|1380150333767262211|#███████ bubbles ...|\n",
      "|  #biden|1380150113650274308|Nothing to see he...|\n",
      "|#vaccine|1380150618275389448|Well that was qui...|\n",
      "|#vaccine|1380150554974904321|Good morning, Twi...|\n",
      "|#vaccine|1380150530526306314|Here for my secon...|\n",
      "|#vaccine|1380150486339366928|You don't have a ...|\n",
      "|#vaccine|1380150434208358402|Because only cert...|\n",
      "|#vaccine|1380150386863013894|#███████\n",
      "#███████...|\n",
      "|#vaccine|1380150373399339009|Proud to see thou...|\n",
      "|  #covid|1380150776299937795|The pandemic is n...|\n",
      "|  #covid|1380150698948575232|Tandon's next Emp...|\n",
      "|  #covid|1380150671123623942|2 days virtual Co...|\n",
      "|  #covid|1380150657823469573|Toronto ICU doc: ...|\n",
      "|  #covid|1380150622796742658|India records 4th...|\n",
      "|  #covid|1380150957300969473|When #███████ hit...|\n",
      "|  #covid|1380150954503381000|With low literacy...|\n",
      "|  #covid|1380150949264695304|Republican Lawmak...|\n",
      "|  #covid|1380150948895453184|We haven't return...|\n",
      "|  #covid|1380150940217573382|Listen to the @We...|\n",
      "|  #covid|1380150939751944202|#███████ Mayor Di...|\n",
      "+--------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------------------+\n",
      "|label|           tweet_id|          tweet_text|\n",
      "+-----+-------------------+--------------------+\n",
      "|    1|1380150333767262211| bubbles under su...|\n",
      "|    1|1380150113650274308|Nothing to see he...|\n",
      "|    6|1380150618275389448|Well that was qui...|\n",
      "|    6|1380150554974904321|Good morning, Twi...|\n",
      "|    6|1380150530526306314|Here for my secon...|\n",
      "|    6|1380150486339366928|You don't have a ...|\n",
      "|    6|1380150434208358402|Because only cert...|\n",
      "|    6|1380150386863013894|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ocugen: Pot...|\n",
      "|    6|1380150373399339009|Proud to see thou...|\n",
      "|    5|1380150776299937795|The pandemic is n...|\n",
      "|    5|1380150698948575232|Tandon's next Emp...|\n",
      "|    5|1380150671123623942|2 days virtual Co...|\n",
      "|    5|1380150657823469573|Toronto ICU doc: ...|\n",
      "|    5|1380150622796742658|India records 4th...|\n",
      "|    5|1380150957300969473|When  hit,  saw t...|\n",
      "|    5|1380150954503381000|With low literacy...|\n",
      "|    5|1380150949264695304|Republican Lawmak...|\n",
      "|    5|1380150948895453184|We haven't return...|\n",
      "|    5|1380150940217573382|Listen to the  in...|\n",
      "|    5|1380150939751944202| Mayor Discusses ...|\n",
      "+-----+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when,regexp_replace\n",
    "df2 = df.withColumn(\"label\", when(df.label == \"#biden\",1)\n",
    "                                 .when(df.label == \"#inflation\",2)\n",
    "                    .when(df.label == \"#china\",3)\n",
    "                    .when(df.label == \"#stopasianhate\",4)\n",
    "                    .when(df.label == \"#covid\",5)\n",
    "                    .when(df.label == \"#vaccine\",6)\n",
    "                                 .when(df.label.isNull() ,\"\")\n",
    "                                 .otherwise(df.label))\n",
    "df2 = df2.withColumn('tweet_text', regexp_replace('tweet_text', r'[#@][^\\s#@]+', ''))\n",
    "df2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------------------+--------------------+\n",
      "|label|           tweet_id|          tweet_text|   tweet_text_tokens|\n",
      "+-----+-------------------+--------------------+--------------------+\n",
      "|    1|1380150333767262211| bubbles under su...|[, bubbles, under...|\n",
      "|    1|1380150113650274308|Nothing to see he...|[nothing, to, see...|\n",
      "|    6|1380150618275389448|Well that was qui...|[well, that, was,...|\n",
      "|    6|1380150554974904321|Good morning, Twi...|[good, morning,, ...|\n",
      "|    6|1380150530526306314|Here for my secon...|[here, for, my, s...|\n",
      "|    6|1380150486339366928|You don't have a ...|[you, don't, have...|\n",
      "|    6|1380150434208358402|Because only cert...|[because, only, c...|\n",
      "|    6|1380150386863013894|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ocugen: Pot...|[, , , , , , ocug...|\n",
      "|    6|1380150373399339009|Proud to see thou...|[proud, to, see, ...|\n",
      "|    5|1380150776299937795|The pandemic is n...|[the, pandemic, i...|\n",
      "|    5|1380150698948575232|Tandon's next Emp...|[tandon's, next, ...|\n",
      "|    5|1380150671123623942|2 days virtual Co...|[2, days, virtual...|\n",
      "|    5|1380150657823469573|Toronto ICU doc: ...|[toronto, icu, do...|\n",
      "|    5|1380150622796742658|India records 4th...|[india, records, ...|\n",
      "|    5|1380150957300969473|When  hit,  saw t...|[when, , hit,, , ...|\n",
      "|    5|1380150954503381000|With low literacy...|[with, low, liter...|\n",
      "|    5|1380150949264695304|Republican Lawmak...|[republican, lawm...|\n",
      "|    5|1380150948895453184|We haven't return...|[we, haven't, ret...|\n",
      "|    5|1380150940217573382|Listen to the  in...|[listen, to, the,...|\n",
      "|    5|1380150939751944202| Mayor Discusses ...|[, mayor, discuss...|\n",
      "+-----+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We Tokenize the tweet texts\n",
    "\n",
    "from pyspark.ml.feature import  Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import  IDF\n",
    "final_data = df2.withColumn(\"label\", df2[\"label\"].cast(IntegerType()))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"tweet_text_tokens\")\n",
    "data_words = tokenizer.transform(final_data)\n",
    "data_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------------------+--------------------+--------------------+\n",
      "|label|           tweet_id|          tweet_text|   tweet_text_tokens|       count_vectors|\n",
      "+-----+-------------------+--------------------+--------------------+--------------------+\n",
      "|    1|1380150333767262211| bubbles under su...|[, bubbles, under...|(4102,[0,2,6,7,30...|\n",
      "|    1|1380150113650274308|Nothing to see he...|[nothing, to, see...|(4102,[0,1,2,4,6,...|\n",
      "|    6|1380150618275389448|Well that was qui...|[well, that, was,...|(4102,[0,12,13,33...|\n",
      "|    6|1380150554974904321|Good morning, Twi...|[good, morning,, ...|(4102,[5,11,28,42...|\n",
      "|    6|1380150530526306314|Here for my secon...|[here, for, my, s...|(4102,[0,1,5,7,8,...|\n",
      "|    6|1380150486339366928|You don't have a ...|[you, don't, have...|(4102,[0,2,5,6,11...|\n",
      "|    6|1380150434208358402|Because only cert...|[because, only, c...|(4102,[0,2,3,5,11...|\n",
      "|    6|1380150386863013894|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ocugen: Pot...|[, , , , , , ocug...|(4102,[0,6,31,61,...|\n",
      "|    6|1380150373399339009|Proud to see thou...|[proud, to, see, ...|(4102,[0,2,5,7,9,...|\n",
      "|    5|1380150776299937795|The pandemic is n...|[the, pandemic, i...|(4102,[0,1,2,3,4,...|\n",
      "|    5|1380150698948575232|Tandon's next Emp...|[tandon's, next, ...|(4102,[0,5,6,15,1...|\n",
      "|    5|1380150671123623942|2 days virtual Co...|[2, days, virtual...|(4102,[0,12,26,12...|\n",
      "|    5|1380150657823469573|Toronto ICU doc: ...|[toronto, icu, do...|(4102,[0,1,2,3,4,...|\n",
      "|    5|1380150622796742658|India records 4th...|[india, records, ...|(4102,[0,1,2,3,4,...|\n",
      "|    5|1380150957300969473|When  hit,  saw t...|[when, , hit,, , ...|(4102,[0,1,3,4,5,...|\n",
      "|    5|1380150954503381000|With low literacy...|[with, low, liter...|(4102,[0,1,2,4,10...|\n",
      "|    5|1380150949264695304|Republican Lawmak...|[republican, lawm...|(4102,[0,7,489,18...|\n",
      "|    5|1380150948895453184|We haven't return...|[we, haven't, ret...|(4102,[0,1,2,5,6,...|\n",
      "|    5|1380150940217573382|Listen to the  in...|[listen, to, the,...|(4102,[0,1,2,4,5,...|\n",
      "|    5|1380150939751944202| Mayor Discusses ...|[, mayor, discuss...|(4102,[0,473,771,...|\n",
      "+-----+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(inputCol=\"tweet_text_tokens\", outputCol=\"count_vectors\")\n",
    "count_model = count.fit(data_words)\n",
    "data_count_vectors = count_model.transform(data_words)\n",
    "data_count_vectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|(4102,[0,2,6,7,30...|\n",
      "|    1|(4102,[0,1,2,4,6,...|\n",
      "|    6|(4102,[0,12,13,33...|\n",
      "|    6|(4102,[5,11,28,42...|\n",
      "|    6|(4102,[0,1,5,7,8,...|\n",
      "|    6|(4102,[0,2,5,6,11...|\n",
      "|    6|(4102,[0,2,3,5,11...|\n",
      "|    6|(4102,[0,6,31,61,...|\n",
      "|    6|(4102,[0,2,5,7,9,...|\n",
      "|    5|(4102,[0,1,2,3,4,...|\n",
      "|    5|(4102,[0,5,6,15,1...|\n",
      "|    5|(4102,[0,12,26,12...|\n",
      "|    5|(4102,[0,1,2,3,4,...|\n",
      "|    5|(4102,[0,1,2,3,4,...|\n",
      "|    5|(4102,[0,1,3,4,5,...|\n",
      "|    5|(4102,[0,1,2,4,10...|\n",
      "|    5|(4102,[0,7,489,18...|\n",
      "|    5|(4102,[0,1,2,5,6,...|\n",
      "|    5|(4102,[0,1,2,4,5,...|\n",
      "|    5|(4102,[0,473,771,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"count_vectors\", outputCol=\"features\")\n",
    "idf_model = idf.fit(data_count_vectors)\n",
    "idf_data = idf_model.transform(data_count_vectors)\n",
    "\n",
    "preprocessed_data = idf_data.select(\"label\", \"features\")\n",
    "preprocessed_data = preprocessed_data.withColumn(\"label\", preprocessed_data[\"label\"].cast(IntegerType()))\n",
    "preprocessed_data.show()\n",
    "\n",
    "train_set, test_set = preprocessed_data.randomSplit([0.75,0.25],0)\n",
    "\n",
    "#pandas_df = preprocessed_data.toPandas()\n",
    "#print(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import numpy as np\n",
    "\n",
    "log_model = LogisticRegression(maxIter = 100)\n",
    "#elasticnetPar = 0 : ridge , elasticnetPar = 1 : lasso  regpar = lambda \n",
    "grid = ParamGridBuilder().addGrid(log_model.regParam, np.linspace(0, 100,5)).addGrid(log_model.elasticNetParam, [1]).build()\n",
    "cross_val = CrossValidator(estimator=log_model,\n",
    "                          estimatorParamMaps=grid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds= 2)  \n",
    "cross_val = cross_val.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid=LogisticRegression_7ad9eebcf013, numClasses=7, numFeatures=4102\n"
     ]
    }
   ],
   "source": [
    "best_model = cross_val.bestModel\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|    1|(4102,[0,1,2,3,4,...|[-4.5290954043510...|[4.19404397101991...|       6.0|\n",
      "|    2|(4102,[0,1,2,3,4,...|[-4.0324531998479...|[3.46018048246986...|       6.0|\n",
      "|    2|(4102,[0,1,2,3,5,...|[-4.0778927836054...|[5.31337449709482...|       5.0|\n",
      "|    2|(4102,[0,4,6,7,8,...|[-4.2140712230301...|[1.70555264104222...|       5.0|\n",
      "|    3|(4102,[0,1,2,3,4,...|[-4.0945175511037...|[2.81315466286097...|       3.0|\n",
      "|    3|(4102,[0,1,2,3,4,...|[-5.0544506988865...|[5.35027631699152...|       5.0|\n",
      "|    3|(4102,[0,1,2,3,4,...|[-4.4000560704736...|[3.00549271454315...|       3.0|\n",
      "|    3|(4102,[0,1,2,4,5,...|[-4.5787365590111...|[3.39241221339827...|       3.0|\n",
      "|    3|(4102,[0,1,2,4,6,...|[-4.7701134905650...|[1.32201678093548...|       6.0|\n",
      "|    3|(4102,[0,1,2,4,8,...|[-3.9691032712043...|[6.62593941700903...|       5.0|\n",
      "|    3|(4102,[0,1,3,5,6,...|[-4.5941491666494...|[1.2386890795114E...|       3.0|\n",
      "|    3|(4102,[0,1,4,6,9,...|[-4.0635194332139...|[4.57069527782727...|       6.0|\n",
      "|    3|(4102,[0,1,5,6,7,...|[-3.7684504135118...|[6.70666392718897...|       6.0|\n",
      "|    3|(4102,[0,1,41,177...|[-3.8374341024504...|[2.0432342094679E...|       6.0|\n",
      "|    3|(4102,[0,2,14,16,...|[-3.6065174504352...|[6.32242587739580...|       6.0|\n",
      "|    3|(4102,[0,2,16,37,...|[-3.5892585561634...|[1.75787849783445...|       6.0|\n",
      "|    3|(4102,[0,2,64,164...|[-3.5780931430187...|[6.45658027374830...|       1.0|\n",
      "|    3|(4102,[0,4,6,150,...|[-3.4962113964074...|[0.00169837892511...|       5.0|\n",
      "|    3|(4102,[0,9,56,151...|[-3.7347217592179...|[4.61195165698252...|       1.0|\n",
      "|    3|(4102,[0,10,38,70...|[-3.5895891099831...|[3.69883455405713...|       6.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = best_model.transform(test_set)\n",
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       6.0|    3|\n",
      "|    3|       1.0|    4|\n",
      "|    5|       5.0|   15|\n",
      "|    6|       6.0|   22|\n",
      "|    1|       5.0|    2|\n",
      "|    6|       5.0|    9|\n",
      "|    3|       3.0|    9|\n",
      "|    3|       5.0|   10|\n",
      "|    2|       2.0|    1|\n",
      "|    1|       1.0|    2|\n",
      "|    6|       1.0|    1|\n",
      "|    5|       6.0|   12|\n",
      "|    3|       6.0|   10|\n",
      "|    2|       6.0|    3|\n",
      "|    2|       5.0|    2|\n",
      "|    6|       3.0|    3|\n",
      "|    4|       6.0|    2|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.groupBy('label','prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted recall = 0.44545454545454544\n",
      "Weighted precision = 0.5292326094957674\n",
      "Weighted F1 Score = 0.4279724872828321\n",
      "Accuracy = 0.44545454545454544\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Create both evaluators\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget = best_model.transform(test_set).select(\"label\", \"prediction\")\n",
    "\n",
    "# Get metrics\n",
    "Accuracy = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "WeightedF1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedFMeasure\"})\n",
    "weightedPrecision = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "\n",
    "print(\"Weighted recall = %s\" % weightedRecall)\n",
    "print(\"Weighted precision = %s\" % weightedPrecision)\n",
    "print(\"Weighted F1 Score = %s\" % WeightedF1)\n",
    "print(\"Accuracy = %s\" % Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_model.write().overwrite().save('countvectorizer')\n",
    "idf_model.write().overwrite().save('tf-idf')\n",
    "best_model.write().overwrite().save('log_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from  pyspark.ml.feature import CountVectorizerModel\n",
    "from pyspark.ml.feature import  IDFModel \n",
    "\n",
    "    \n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "  \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_log_model'] = LogisticRegressionModel.load('log_regression')\n",
    "        globals()['my_count_vectoriser'] = CountVectorizerModel.load('countvectorizer')\n",
    "        globals()['my_tf_idf'] = IDFModel.load('tf-idf')        # Replace '***' with:    [...].load('my_logistic_regression')\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model: \n",
    "    \n",
    "    token = tokenizer.transform(df)\n",
    "    count = my_count_vectoriser.transform(token)\n",
    "    \n",
    "    idf = my_tf_idf.transform(count)\n",
    "    idf_select = idf.select(\"label\", \"features\")\n",
    "    predict = my_log_model.transform(idf_select)\n",
    "    predict_names = predict.withColumn(\"prediction\", when(predict.prediction == 1, '#biden')\n",
    "                                 .when(predict.prediction == 2,'#inflation')\n",
    "                    .when(predict.prediction == 3,'#china')\n",
    "                    .when(predict.prediction == 4,\"#stopasianhate\")\n",
    "                    .when(predict.prediction == 5,\"#covid\")\n",
    "                    .when(predict.prediction == 6,\"#vaccine\")\n",
    "                                 .when(predict.prediction.isNull() ,\"\")\n",
    "                                 .otherwise(predict.prediction))\n",
    "    predict_names.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2021-04-29 11:33:40 =========\n",
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#covid|1387699178638356481|The pandemic has ...|\n",
      "|#covid|1387699166906888193|Join us on 18 May...|\n",
      "+------+-------------------+--------------------+\n",
      "\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "| label|            features|       rawPrediction|         probability|prediction|\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "|#covid|(4102,[1,2,5,11,1...|[-4.0291202056636...|[3.58548742211234...|    #covid|\n",
      "|#covid|(4102,[0,1,2,7,9,...|[-4.0289907259607...|[7.15337731391432...|  #vaccine|\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2021-04-29 11:34:00 =========\n",
      "+--------------+-------------------+--------------------+\n",
      "|         label|           tweet_id|          tweet_text|\n",
      "+--------------+-------------------+--------------------+\n",
      "|#stopasianhate|1387699324201672706|#███████  #██████...|\n",
      "+--------------+-------------------+--------------------+\n",
      "\n",
      "+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|         label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|#stopasianhate|(4102,[0,5,6,7,10...|[-3.7917761250536...|[2.89519592060247...|    #covid|\n",
      "+--------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2021-04-29 11:34:10 =========\n",
      "+--------+-------------------+--------------------+\n",
      "|   label|           tweet_id|          tweet_text|\n",
      "+--------+-------------------+--------------------+\n",
      "|#vaccine|1387699640057925635|Stop whining, fol...|\n",
      "+--------+-------------------+--------------------+\n",
      "\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|   label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|#vaccine|(4102,[0,9,23,25,...|[-3.9131207905894...|[4.20762217615649...|  #vaccine|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2021-04-29 11:34:20 =========\n",
      "+--------+-------------------+--------------------+\n",
      "|   label|           tweet_id|          tweet_text|\n",
      "+--------+-------------------+--------------------+\n",
      "|#vaccine|1387699483685883909|UK - #███████ lea...|\n",
      "+--------+-------------------+--------------------+\n",
      "\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|   label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|#vaccine|(4102,[1,3,8,13,2...|[-4.1801997938843...|[7.6113759542853E...|  #vaccine|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2021-04-29 11:34:30 =========\n",
      "+--------+-------------------+--------------------+\n",
      "|   label|           tweet_id|          tweet_text|\n",
      "+--------+-------------------+--------------------+\n",
      "|#vaccine|1387699836447825920|All areas #██████...|\n",
      "+--------+-------------------+--------------------+\n",
      "\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|   label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|#vaccine|(4102,[34,2206],[...|[-3.4212141662109...|[0.00156549869348...|    #china|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2021-04-29 11:34:40 =========\n",
      "+--------+-------------------+--------------------+\n",
      "|   label|           tweet_id|          tweet_text|\n",
      "+--------+-------------------+--------------------+\n",
      "|#vaccine|1387699819016245249|I was collecting ...|\n",
      "+--------+-------------------+--------------------+\n",
      "\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|   label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|#vaccine|(4102,[3,5,28,32,...|[-4.3293673279142...|[1.89924868933224...|  #vaccine|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "========= 2021-04-29 11:34:50 =========\n",
      "+--------+-------------------+---------------------+\n",
      "|   label|           tweet_id|           tweet_text|\n",
      "+--------+-------------------+---------------------+\n",
      "|#vaccine|1387699759209541632|WTACH VIDEO：Nearl...|\n",
      "+--------+-------------------+---------------------+\n",
      "\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|   label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|#vaccine|(4102,[4,61,270,3...|[-3.5902960378938...|[2.19283957034341...|  #vaccine|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
