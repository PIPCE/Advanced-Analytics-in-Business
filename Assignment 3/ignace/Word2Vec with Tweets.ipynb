{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import threading\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import Row\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql.functions import split, udf, struct, array, col, lit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import word2vecUtilities as wvu\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper thread in order to have a stream running in the background in Jupyter\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2Vec and xgboost with Tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## need C++ 2019 to load in gensim package of google \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2029"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.load(\"file:///C:/Users/ignac/Documents/data/dataframe2\")\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+-------------------+----------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                               value|           tweet_id|           label|                                                                                         tweet_text1|                                                                                          tweet_text|\n",
      "+----------------------------------------------------------------------------------------------------+-------------------+----------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|{\"tweet_id\": 1389090870696185859, \"tweet_text\": \"@2ordinary1 @claudiakitty_g @juslilmissp @Mariso...|1389090870696185859|        \"#biden\"|{\"@2ordinary1 @claudiakitty_g @juslilmissp @MarisolTorresRS @summerisunique @WokeAnimal @Martinsu...|{\"                                                 https://t.co/ni9DVtZTec\\n\\nThank you very much...|\n",
      "|{\"tweet_id\": 1388991974263463937, \"tweet_text\": \"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 #\\u2...|1388991974263463937|        \"#china\"|{\"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 #\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\n#\\u258...|                {\"  # poets   uality\\u00a0                  # india      https://t.co/67EG78DNj3\", }|\n",
      "|{\"tweet_id\": 1389083482320560130, \"tweet_text\": \"TSMC says can catch up with auto chip demand by ...|1389083482320560130|        \"#china\"|{\"TSMC says can catch up with auto chip demand by end June -CBS\\n#\\u2588\\u2588\\u2588\\u2588\\u2588\\...|        {\"TSMC says can catch up with auto chip demand by end June -CBS\\n                          }|\n",
      "|{\"tweet_id\": 1389098504778403841, \"tweet_text\": \"\\ud83d\\udea8\\ud83d\\udea8\\ud83d\\udea8BREAKING NEW...|1389098504778403841|    \"#inflation\"|{\"\\ud83d\\udea8\\ud83d\\udea8\\ud83d\\udea8BREAKING NEWS \\ud83d\\udea8\\ud83d\\udea8\\ud83d\\udea8\\n\\n\\ud83...|{\"\\ud83d\\udea8\\ud83d\\udea8\\ud83d\\udea8BREAKING NEWS \\ud83d\\udea8\\ud83d\\udea8\\ud83d\\udea8\\n\\n\\ud83...|\n",
      "|{\"tweet_id\": 1389079721766391813, \"tweet_text\": \"It was a heart, it broke \\u2764\\ufe0f\\ud83d\\udc9...|1389079721766391813|        \"#covid\"|{\"It was a heart, it broke \\u2764\\ufe0f\\ud83d\\udc94\\ud83d\\udc94\\ud83e\\udd7a\\ud83e\\udd7a\\n#\\u2588\\...|{\"It was a heart, it broke \\u2764\\ufe0f\\ud83d\\udc94\\ud83d\\udc94\\ud83e\\udd7a\\ud83e\\udd7a\\n        ...|\n",
      "|{\"tweet_id\": 1389053238268227584, \"tweet_text\": \"Our liberty depends on freedom of the press.\\n.\\...|1389053238268227584|        \"#covid\"|{\"Our liberty depends on freedom of the press.\\n.\\n.\\n.\\n#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2...|               {\"Our liberty depends on freedom of the press.\\n.\\n.\\n.\\n https://t.co/vf5v1WzNc8\", }|\n",
      "|{\"tweet_id\": 1389069630895915013, \"tweet_text\": \"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 Anal...|1389069630895915013|      \"#vaccine\"|{\"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 Analysis Halfway Mark\\n#\\u2588\\u2588\\u2588\\u2588\\u2...|               {\" Analysis Halfway Mark\\n       \\n   \\n        \\n        https://t.co/oZmeoRvsTk\", }|\n",
      "|{\"tweet_id\": 1389070267964542976, \"tweet_text\": \"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 Anal...|1389070267964542976|      \"#vaccine\"|{\"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 Analysis Halfway Mark\\n#\\u2588\\u2588\\u2588\\u2588\\u2...|               {\" Analysis Halfway Mark\\n       \\n   \\n        \\n        https://t.co/6KNcqLrgTA\", }|\n",
      "|{\"tweet_id\": 1389086915735932928, \"tweet_text\": \"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 Anal...|1389086915735932928|      \"#vaccine\"|{\"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 Analysis Halfway Mark\\n#\\u2588\\u2588\\u2588\\u2588\\u2...|               {\" Analysis Halfway Mark\\n       \\n   \\n        \\n        https://t.co/yGQtiNaFtI\", }|\n",
      "|{\"tweet_id\": 1389041057581522949, \"tweet_text\": \"\\ud83d\\udc49@knowledge_fact_techz \\ud83d\\udc48\\n...|1389041057581522949|        \"#china\"|{\"\\ud83d\\udc49@knowledge_fact_techz \\ud83d\\udc48\\n\\ud83d\\udc9f Follow me for amazing facts.\\n\\ud8...|{\"\\ud83d\\udc49 \\ud83d\\udc48\\n\\ud83d\\udc9f Follow me for amazing facts.\\n\\ud83c\\udd95Fact by trans...|\n",
      "|{\"tweet_id\": 1389102679507423234, \"tweet_text\": \"\\ud83c\\udd7f\\ufe0f\\ud83c\\udd71\\ufe0f\\n\\n\\u25b6\\u...|1389102679507423234|        \"#china\"|{\"\\ud83c\\udd7f\\ufe0f\\ud83c\\udd71\\ufe0f\\n\\n\\u25b6\\ufe0f Daily Video News : #\\u2588\\u2588\\u2588\\u25...|{\"\\ud83c\\udd7f\\ufe0f\\ud83c\\udd71\\ufe0f\\n\\n\\u25b6\\ufe0f Daily Video News :       Offers($/t):     ...|\n",
      "|{\"tweet_id\": 1389031221326356480, \"tweet_text\": \"@nila3333 @jyotsnadevi33 @ajitsinghpundir @colkt...|1389031221326356480|        \"#china\"|{\"@nila3333 @jyotsnadevi33 @ajitsinghpundir @colkt @GenPanwar @PashaSehgal @Chouhan_ks @wetwokris...|{\"                                  Suprabhat  \\nGood morning all, \\n\\nIndian response to  keep y...|\n",
      "|{\"tweet_id\": 1389018103225458689, \"tweet_text\": \"You Wear It Well \\ud83d\\ude0d\\nhttps://t.co/VEzJ...|1389018103225458689|\"#stopasianhate\"|{\"You Wear It Well \\ud83d\\ude0d\\nhttps://t.co/VEzJBkKuzF\\n\\n#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588...|{\"You Wear It Well \\ud83d\\ude0d\\nhttps://t.co/VEzJBkKuzF\\n\\n                    https://t.co/GQcI...|\n",
      "|{\"tweet_id\": 1389063978798796800, \"tweet_text\": \"Double DOWN ...on the BANG \\ud83c\\uddfa\\ud83c\\ud...|1389063978798796800|        \"#biden\"|{\"Double DOWN ...on the BANG \\ud83c\\uddfa\\ud83c\\uddf8                                         #\\u...|{\"Double DOWN ...on the BANG \\ud83c\\uddfa\\ud83c\\uddf8                                            ...|\n",
      "|{\"tweet_id\": 1389064306839343104, \"tweet_text\": \"@JohnCena Here is your #\\u2588\\u2588\\u2588\\u2588...|1389064306839343104|        \"#biden\"|{\"@JohnCena Here is your #\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 #\\u2588\\u2588\\u2588\\u2588\\u2...|{\" Here is your   Phillies Diaz   Simmons  Jonathan Villar Tupelo Marcus Smart Verdejo David Hale...|\n",
      "|{\"tweet_id\": 1389030124985602048, \"tweet_text\": \"WNBA star Stewart engaged to Mercury's Xargay\\n#...|1389030124985602048|        \"#biden\"|{\"WNBA star Stewart engaged to Mercury's Xargay\\n#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 #\\u2...|                              {\"WNBA star Stewart engaged to Mercury's Xargay\\n                    }|\n",
      "|{\"tweet_id\": 1389054995476086785, \"tweet_text\": \"Throw Pillow\\nJust $18.63\\n\\nVisit us on Redbubb...|1389054995476086785|        \"#covid\"|{\"Throw Pillow\\nJust $18.63\\n\\nVisit us on Redbubble - click below\\nhttps://t.co/pSZLAvEZzw\\n\\n#\\...|{\"Throw Pillow\\nJust $18.63\\n\\nVisit us on Redbubble - click below\\nhttps://t.co/pSZLAvEZzw\\n\\n  ...|\n",
      "|{\"tweet_id\": 1389037501797523459, \"tweet_text\": \"@nitin_gadkari @umasribharti 100% Treatment for ...|1389037501797523459|        \"#covid\"|{\"@nitin_gadkari @umasribharti 100% Treatment for Covid Patients, please Listen Again &amp; Again...|{\"  100% Treatment for Covid Patients, please Listen Again &amp; Again\\n           of            ...|\n",
      "|{\"tweet_id\": 1389014354285273088, \"tweet_text\": \"\\\"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 #\\...|1389014354285273088|      \"#vaccine\"|{\"\\\"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 #\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\\" #\\u...|{\"\\\"   (  MD or more fittingly,       etc....\\n\\nhttps://t.co/OQcvvKMD0o\\n\\n        19  https://t...|\n",
      "|{\"tweet_id\": 1389037006118957059, \"tweet_text\": \"\\\"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 #\\...|1389037006118957059|        \"#covid\"|{\"\\\"#\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588 #\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\\" #\\u...|{\"\\\"   (  MD or more fittingly,       etc....\\n\\nhttps://t.co/OQcvvKMD0o\\n\\n        19  https://t...|\n",
      "+----------------------------------------------------------------------------------------------------+-------------------+----------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when,regexp_replace\n",
    "df2 = df.withColumn(\"label\", when(df.label == \"#biden\",1)\n",
    "                                 .when(df.label == \"#inflation\",2)\n",
    "                    .when(df.label == \"#china\",3)\n",
    "                    .when(df.label == \"#stopasianhate\",4)\n",
    "                    .when(df.label == \"#covid\",5)\n",
    "                    .when(df.label == \"#vaccine\",6)\n",
    "                                 .when(df.label.isNull() ,\"\")\n",
    "                                 .otherwise(df.label))\n",
    "df2 = df2.withColumn('tweet_text', regexp_replace('tweet_text', r'[#@][^\\s#@]+', ''))\n",
    "df2.show(20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|               value|           tweet_id|           label|         tweet_text1|          tweet_text|   tweet_text_tokens|\n",
      "+--------------------+-------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|{\"tweet_id\": 1389...|1389090870696185859|        \"#biden\"|{\"@2ordinary1 @cl...|{\"               ...|[{\", , , , , , , ...|\n",
      "|{\"tweet_id\": 1388...|1388991974263463937|        \"#china\"|{\"#\\u2588\\u2588\\u...|{\"  # poets   ual...|[{\", , #, poets, ...|\n",
      "|{\"tweet_id\": 1389...|1389083482320560130|        \"#china\"|{\"TSMC says can c...|{\"TSMC says can c...|[{\"tsmc, says, ca...|\n",
      "|{\"tweet_id\": 1389...|1389098504778403841|    \"#inflation\"|{\"\\ud83d\\udea8\\ud...|{\"\\ud83d\\udea8\\ud...|[{\"\\ud83d\\udea8\\u...|\n",
      "|{\"tweet_id\": 1389...|1389079721766391813|        \"#covid\"|{\"It was a heart,...|{\"It was a heart,...|[{\"it, was, a, he...|\n",
      "|{\"tweet_id\": 1389...|1389053238268227584|        \"#covid\"|{\"Our liberty dep...|{\"Our liberty dep...|[{\"our, liberty, ...|\n",
      "|{\"tweet_id\": 1389...|1389069630895915013|      \"#vaccine\"|{\"#\\u2588\\u2588\\u...|{\" Analysis Halfw...|[{\", analysis, ha...|\n",
      "|{\"tweet_id\": 1389...|1389070267964542976|      \"#vaccine\"|{\"#\\u2588\\u2588\\u...|{\" Analysis Halfw...|[{\", analysis, ha...|\n",
      "|{\"tweet_id\": 1389...|1389086915735932928|      \"#vaccine\"|{\"#\\u2588\\u2588\\u...|{\" Analysis Halfw...|[{\", analysis, ha...|\n",
      "|{\"tweet_id\": 1389...|1389041057581522949|        \"#china\"|{\"\\ud83d\\udc49@kn...|{\"\\ud83d\\udc49 \\u...|[{\"\\ud83d\\udc49, ...|\n",
      "|{\"tweet_id\": 1389...|1389102679507423234|        \"#china\"|{\"\\ud83c\\udd7f\\uf...|{\"\\ud83c\\udd7f\\uf...|[{\"\\ud83c\\udd7f\\u...|\n",
      "|{\"tweet_id\": 1389...|1389031221326356480|        \"#china\"|{\"@nila3333 @jyot...|{\"               ...|[{\", , , , , , , ...|\n",
      "|{\"tweet_id\": 1389...|1389018103225458689|\"#stopasianhate\"|{\"You Wear It Wel...|{\"You Wear It Wel...|[{\"you, wear, it,...|\n",
      "|{\"tweet_id\": 1389...|1389063978798796800|        \"#biden\"|{\"Double DOWN ......|{\"Double DOWN ......|[{\"double, down, ...|\n",
      "|{\"tweet_id\": 1389...|1389064306839343104|        \"#biden\"|{\"@JohnCena Here ...|{\" Here is your  ...|[{\", here, is, yo...|\n",
      "|{\"tweet_id\": 1389...|1389030124985602048|        \"#biden\"|{\"WNBA star Stewa...|{\"WNBA star Stewa...|[{\"wnba, star, st...|\n",
      "|{\"tweet_id\": 1389...|1389054995476086785|        \"#covid\"|{\"Throw Pillow\\nJ...|{\"Throw Pillow\\nJ...|[{\"throw, pillow\\...|\n",
      "|{\"tweet_id\": 1389...|1389037501797523459|        \"#covid\"|{\"@nitin_gadkari ...|{\"  100% Treatmen...|[{\", , 100%, trea...|\n",
      "|{\"tweet_id\": 1389...|1389014354285273088|      \"#vaccine\"|{\"\\\"#\\u2588\\u2588...|{\"\\\"   (  MD or m...|[{\"\\\", , , (, , m...|\n",
      "|{\"tweet_id\": 1389...|1389037006118957059|        \"#covid\"|{\"\\\"#\\u2588\\u2588...|{\"\\\"   (  MD or m...|[{\"\\\", , , (, , m...|\n",
      "+--------------------+-------------------+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We Tokenize the tweet texts\n",
    "\n",
    "from pyspark.ml.feature import  Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import  IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"tweet_text_tokens\")\n",
    "data_words = tokenizer.transform(df2)\n",
    "data_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value                  object\n",
      "tweet_id               object\n",
      "label                category\n",
      "tweet_text1            object\n",
      "tweet_text             object\n",
      "tweet_text_tokens      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = data_words.toPandas()\n",
    "\n",
    "\n",
    "\n",
    "df.label = df.label.astype('category')\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = df['label'].cat.categories\n",
    "df['label'] = df['label'].cat.codes\n",
    "X = df['tweet_text_tokens']\n",
    "y = df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import call\n",
    "test_size = 0.2\n",
    "random_state = 1234\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1623"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "class GensimWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Word vectors are averaged across to create the document-level vectors/features.\n",
    "    gensim's own gensim.sklearn_api.W2VTransformer doesn't support out of vocabulary words,\n",
    "    hence we roll out our own.\n",
    "    All the parameters are gensim.models.Word2Vec's parameters.\n",
    "    https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size=300, alpha=0.25, window=40, min_count=7, max_vocab_size=None,\n",
    "                 sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5,\n",
    "                 ns_exponent=0.75, cbow_mean=1, hashfxn=hash, iter=10, null_word=0,\n",
    "                 trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False,\n",
    "                 callbacks=(), max_final_vocab=None):\n",
    "        self.size = size\n",
    "        self.alpha = alpha\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.sample = sample\n",
    "        self.seed = seed\n",
    "        self.workers = workers\n",
    "        self.min_alpha = min_alpha\n",
    "        self.sg = sg\n",
    "        self.hs = hs\n",
    "        self.negative = negative\n",
    "        self.ns_exponent = ns_exponent\n",
    "        self.cbow_mean = cbow_mean\n",
    "        self.hashfxn = hashfxn\n",
    "        self.iter = iter\n",
    "        self.null_word = null_word\n",
    "        self.trim_rule = trim_rule\n",
    "        self.sorted_vocab = sorted_vocab\n",
    "        self.batch_words = batch_words\n",
    "        self.compute_loss = compute_loss\n",
    "        self.callbacks = callbacks\n",
    "        self.max_final_vocab = max_final_vocab\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model_ = Word2Vec(\n",
    "            sentences=X, corpus_file=None,\n",
    "            size=self.size, alpha=self.alpha, window=self.window, min_count=self.min_count,\n",
    "            max_vocab_size=self.max_vocab_size, sample=self.sample, seed=self.seed,\n",
    "            workers=self.workers, min_alpha=self.min_alpha, sg=self.sg, hs=self.hs,\n",
    "            negative=self.negative, ns_exponent=self.ns_exponent, cbow_mean=self.cbow_mean,\n",
    "            hashfxn=self.hashfxn, iter=self.iter, null_word=self.null_word,\n",
    "            trim_rule=self.trim_rule, sorted_vocab=self.sorted_vocab, batch_words=self.batch_words,\n",
    "            compute_loss=self.compute_loss, callbacks=self.callbacks,\n",
    "            max_final_vocab=self.max_final_vocab)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_embeddings = np.array([self._get_embedding(words) for words in X])\n",
    "        return X_embeddings\n",
    "\n",
    "    def _get_embedding(self, words):\n",
    "        valid_words = [word for word in words if word in self.model_.wv.vocab]\n",
    "        if valid_words:\n",
    "            embedding = np.zeros((len(valid_words), self.size), dtype=np.float32)\n",
    "            for idx, word in enumerate(valid_words):\n",
    "                embedding[idx] = self.model_.wv[word]\n",
    "\n",
    "            return np.mean(embedding, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('w2v', GensimWord2VecVectorizer(sg=1)),\n",
       "                ('xgb',\n",
       "                 XGBClassifier(base_score=None, booster=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=None, gamma=None, gpu_id=None,\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints=None, learning_rate=0.3,\n",
       "                               max_delta_step=None, max_depth=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=-1, num_parallel_tree=None,\n",
       "                               random_state=None, reg_alpha=None,\n",
       "                               reg_lambda=None, scale_pos_weight=None,\n",
       "                               subsample=None, tree_method=None,\n",
       "                               validate_parameters=None, verbosity=None))])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "gensim_word2vec_tr = GensimWord2VecVectorizer(size=300, min_count=7, sg=1, alpha=0.25, iter=10)\n",
    "xgb = XGBClassifier(learning_rate=0.3, n_estimators=100, n_jobs=-1)\n",
    "w2v_xgb = Pipeline([\n",
    "    ('w2v', gensim_word2vec_tr), \n",
    "    ('xgb', xgb)\n",
    "])\n",
    "w2v_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:31:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('w2v', GensimWord2VecVectorizer(sg=1)),\n",
       "                ('xgb',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints='', learning_rate=0.3,\n",
       "                               max_delta_step=0, max_depth=6,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=-1, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "                               subsample=1, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w2v_xgb.fit(X_train, y_train)\n",
    "\n",
    "w2v_xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy 0.9926062846580407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[193,   5,   0,   0,   0,   0],\n",
       "       [  2, 514,   0,   0,   0,   0],\n",
       "       [  1,   0, 410,   0,   0,   1],\n",
       "       [  0,   1,   0,  49,   0,   0],\n",
       "       [  0,   0,   0,   0, 114,   0],\n",
       "       [  1,   1,   0,   0,   0, 331]], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_train_pred = w2v_xgb.predict(X_train)\n",
    "print('Training set accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy 0.5049261083743842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14, 18, 10,  0,  1,  6],\n",
       "       [ 2, 96, 22,  0,  2,  7],\n",
       "       [ 8, 30, 45,  1,  2, 17],\n",
       "       [ 1,  4,  6,  1,  1,  0],\n",
       "       [ 3,  7,  7,  0,  7,  5],\n",
       "       [ 3,  9, 28,  0,  1, 42]], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_test_pred = w2v_xgb.predict(X_test)\n",
    "print('Test set accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
    "confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('he', 0.4268295168876648),\n",
       " ('done', 0.40846240520477295),\n",
       " ('administration', 0.3585570454597473),\n",
       " ('joe', 0.3476449251174927),\n",
       " ('trillion', 0.33165860176086426),\n",
       " ('ask', 0.32433393597602844),\n",
       " ('cases', 0.3111940026283264),\n",
       " ('white', 0.30772295594215393),\n",
       " ('make', 0.3074878752231598),\n",
       " ('agree', 0.30333948135375977)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "vocab_size = len(w2v_xgb.named_steps['w2v'].model_.wv.index2word)\n",
    "print('vocabulary size:', vocab_size)\n",
    "w2v_xgb.named_steps['w2v'].model_.wv.most_similar(positive=['biden'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from  pyspark.ml.feature import CountVectorizerModel\n",
    "from pyspark.ml.feature import  IDFModel \n",
    "import numpy.core.defchararray as np_f\n",
    "\n",
    "    \n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "  \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    df2 = df.withColumn(\"label\", when(df.label == \"#biden\",1)\n",
    "                                 .when(df.label == \"#inflation\",2)\n",
    "                    .when(df.label == \"#china\",3)\n",
    "                    .when(df.label == \"#stopasianhate\",4)\n",
    "                    .when(df.label == \"#covid\",5)\n",
    "                    .when(df.label == \"#vaccine\",6)\n",
    "                                 .when(df.label.isNull() ,\"\")\n",
    "                                 .otherwise(df.label))\n",
    "    df2 = df2.withColumn('tweet_text', regexp_replace('tweet_text', r'[#@][^\\s#@]+', ''))\n",
    "\n",
    "\n",
    "    tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"tweet_text_tokens\")\n",
    "    data_words = tokenizer.transform(df2)\n",
    "    data_words.show()\n",
    "    \n",
    "    DF = data_words.toPandas()\n",
    "    X = DF['tweet_text_tokens']\n",
    "    result = w2v_xgb.predict(X)\n",
    "    result_pd = pd.DataFrame(result)\n",
    "    pred = spark.createDataFrame(result_pd)\n",
    "    result =  pred.selectExpr(\"0 as prediction\")\n",
    "    result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
