{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import threading\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import Row\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql.functions import split, udf, struct, array, col, lit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import word2vecUtilities as wvu\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper thread in order to have a stream running in the background in Jupyter\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2Vec and xgboost with Tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## need C++ 2019 to load in gensim package of google \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+\n",
      "|   label|           tweet_id|          tweet_text|\n",
      "+--------+-------------------+--------------------+\n",
      "|  #biden|1380150333767262211|#███████ bubbles ...|\n",
      "|  #biden|1380150113650274308|Nothing to see he...|\n",
      "|#vaccine|1380150618275389448|Well that was qui...|\n",
      "|#vaccine|1380150554974904321|Good morning, Twi...|\n",
      "|#vaccine|1380150530526306314|Here for my secon...|\n",
      "|#vaccine|1380150486339366928|You don't have a ...|\n",
      "|#vaccine|1380150434208358402|Because only cert...|\n",
      "|#vaccine|1380150386863013894|#███████\n",
      "#███████...|\n",
      "|#vaccine|1380150373399339009|Proud to see thou...|\n",
      "|  #covid|1380150776299937795|The pandemic is n...|\n",
      "|  #covid|1380150698948575232|Tandon's next Emp...|\n",
      "|  #covid|1380150671123623942|2 days virtual Co...|\n",
      "|  #covid|1380150657823469573|Toronto ICU doc: ...|\n",
      "|  #covid|1380150622796742658|India records 4th...|\n",
      "|  #covid|1380150957300969473|When #███████ hit...|\n",
      "|  #covid|1380150954503381000|With low literacy...|\n",
      "|  #covid|1380150949264695304|Republican Lawmak...|\n",
      "|  #covid|1380150948895453184|We haven't return...|\n",
      "|  #covid|1380150940217573382|Listen to the @We...|\n",
      "|  #covid|1380150939751944202|#███████ Mayor Di...|\n",
      "+--------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = sc.textFile('file:///C:/Users/ignac/Documents/GitHub/Advanced-Analytics-in-Business/spark/code_id_idf/lots_of_data/tweets')\n",
    "\n",
    "data\n",
    "\n",
    "df = spark.read.json(data)\n",
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------------------------------------------------------------------------------------------------+\n",
      "|label|           tweet_id|                                                                                          tweet_text|\n",
      "+-----+-------------------+----------------------------------------------------------------------------------------------------+\n",
      "|    1|1380150333767262211|  bubbles under surface....\n",
      "\n",
      " Backs  but Some Call for a Clearer Warning to  https://t.co/56tmzwLaS0|\n",
      "|    1|1380150113650274308|Nothing to see here...Five people killed in  including 2 children, by a gunman, who has been capt...|\n",
      "|    6|1380150618275389448|                                Well that was quick!! Poked by Pfizer...     https://t.co/7jJfkHc0xf|\n",
      "|    6|1380150554974904321|        Good morning, Twitterverse. My kids are getting their first shots today, and I am ecstatic! |\n",
      "|    6|1380150530526306314|      Here for my second   jab and the place is hopping!! Excellent!! Yay    https://t.co/9RuTTOwwLo|\n",
      "|    6|1380150486339366928|You don't have a wank and hold up a card to say you've had a Flu jab ! \n",
      "So why are people holding...|\n",
      "|    6|1380150434208358402|Because only certain individuals are eligible to receive their   young people who are being vacci...|\n",
      "|    6|1380150386863013894|   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ocugen: Potential Covid-19 Vaccine EUA Warrants a Buy, Says Analyst https://t.co/GACGW9FBCn|\n",
      "|    6|1380150373399339009|Proud to see thought leaders   and   teaming up for  session on   Hesitancy.   https://t.co/Z9Pj7...|\n",
      "|    5|1380150776299937795|The pandemic is not over, and water for hygiene is one of the primary ways to fend off  Yet milli...|\n",
      "|    5|1380150698948575232|Tandon's next Empower Hour will feature help from our Career Services team! We will be discussing...|\n",
      "|    5|1380150671123623942|2 days virtual Community Oncology Conference by COA 2021.\n",
      "April 8-9, 2021, 8:30 am - 5:00 pm (EDT...|\n",
      "|    5|1380150657823469573|Toronto ICU doc: \"The avg. cost of an ICU bed in Cda is $3592/d. The cost of paid sick leave for ...|\n",
      "|    5|1380150622796742658|India records 4th highest active cases in the world. IMA former general secretary R.N Tandon says...|\n",
      "|    5|1380150957300969473|When  hit,  saw the need for delivery soar and Rakesh Patel, Executive Director of PM, Hospitalit...|\n",
      "|    5|1380150954503381000|With low literacy &amp; TV penetration, &amp; poor institutional birth rate, Nuh district in Hary...|\n",
      "|    5|1380150949264695304|                       Republican Lawmakers Press  for   Incentive Guidance. https://t.co/JQwJ0oNvPd|\n",
      "|    5|1380150948895453184|We haven't returned to normal, yet but the shift is beginning. You may be ready &amp; anxious! Re...|\n",
      "|    5|1380150940217573382|Listen to the  interview with Drs. Iris Feinberg and Mary Helen O'Connor, where they discuss the ...|\n",
      "|    5|1380150939751944202|                                                  Mayor Discusses  Numbers,  https://t.co/pqCexC2mh3|\n",
      "+-----+-------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when,regexp_replace\n",
    "df2 = df.withColumn(\"label\", when(df.label == \"#biden\",1)\n",
    "                                 .when(df.label == \"#inflation\",2)\n",
    "                    .when(df.label == \"#china\",3)\n",
    "                    .when(df.label == \"#stopasianhate\",4)\n",
    "                    .when(df.label == \"#covid\",5)\n",
    "                    .when(df.label == \"#vaccine\",6)\n",
    "                                 .when(df.label.isNull() ,\"\")\n",
    "                                 .otherwise(df.label))\n",
    "df2 = df2.withColumn('tweet_text', regexp_replace('tweet_text', r'[#@][^\\s#@]+', ''))\n",
    "df2.show(20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------------------+--------------------+\n",
      "|label|           tweet_id|          tweet_text|   tweet_text_tokens|\n",
      "+-----+-------------------+--------------------+--------------------+\n",
      "|    1|1380150333767262211| bubbles under su...|[, bubbles, under...|\n",
      "|    1|1380150113650274308|Nothing to see he...|[nothing, to, see...|\n",
      "|    6|1380150618275389448|Well that was qui...|[well, that, was,...|\n",
      "|    6|1380150554974904321|Good morning, Twi...|[good, morning,, ...|\n",
      "|    6|1380150530526306314|Here for my secon...|[here, for, my, s...|\n",
      "|    6|1380150486339366928|You don't have a ...|[you, don't, have...|\n",
      "|    6|1380150434208358402|Because only cert...|[because, only, c...|\n",
      "|    6|1380150386863013894|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ocugen: Pot...|[, , , , , , ocug...|\n",
      "|    6|1380150373399339009|Proud to see thou...|[proud, to, see, ...|\n",
      "|    5|1380150776299937795|The pandemic is n...|[the, pandemic, i...|\n",
      "|    5|1380150698948575232|Tandon's next Emp...|[tandon's, next, ...|\n",
      "|    5|1380150671123623942|2 days virtual Co...|[2, days, virtual...|\n",
      "|    5|1380150657823469573|Toronto ICU doc: ...|[toronto, icu, do...|\n",
      "|    5|1380150622796742658|India records 4th...|[india, records, ...|\n",
      "|    5|1380150957300969473|When  hit,  saw t...|[when, , hit,, , ...|\n",
      "|    5|1380150954503381000|With low literacy...|[with, low, liter...|\n",
      "|    5|1380150949264695304|Republican Lawmak...|[republican, lawm...|\n",
      "|    5|1380150948895453184|We haven't return...|[we, haven't, ret...|\n",
      "|    5|1380150940217573382|Listen to the  in...|[listen, to, the,...|\n",
      "|    5|1380150939751944202| Mayor Discusses ...|[, mayor, discuss...|\n",
      "+-----+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We Tokenize the tweet texts\n",
    "\n",
    "from pyspark.ml.feature import  Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import  IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"tweet_text_tokens\")\n",
    "data_words = tokenizer.transform(df2)\n",
    "data_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label                category\n",
      "tweet_id                int64\n",
      "tweet_text             object\n",
      "tweet_text_tokens      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = data_words.toPandas()\n",
    "\n",
    "\n",
    "\n",
    "df.label = df.label.astype('category')\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = df['label'].cat.categories\n",
    "df['label'] = df['label'].cat.codes\n",
    "X = df['tweet_text_tokens']\n",
    "y = df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import call\n",
    "test_size = 0.2\n",
    "random_state = 1234\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "class GensimWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Word vectors are averaged across to create the document-level vectors/features.\n",
    "    gensim's own gensim.sklearn_api.W2VTransformer doesn't support out of vocabulary words,\n",
    "    hence we roll out our own.\n",
    "    All the parameters are gensim.models.Word2Vec's parameters.\n",
    "    https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size=300, alpha=0.1, window=10, min_count=10, max_vocab_size=None,\n",
    "                 sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5,\n",
    "                 ns_exponent=0.75, cbow_mean=1, hashfxn=hash, iter=10, null_word=0,\n",
    "                 trim_rule=None, sorted_vocab=1, batch_words=1000, compute_loss=False,\n",
    "                 callbacks=(), max_final_vocab=None):\n",
    "        self.size = size\n",
    "        self.alpha = alpha\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.sample = sample\n",
    "        self.seed = seed\n",
    "        self.workers = workers\n",
    "        self.min_alpha = min_alpha\n",
    "        self.sg = sg\n",
    "        self.hs = hs\n",
    "        self.negative = negative\n",
    "        self.ns_exponent = ns_exponent\n",
    "        self.cbow_mean = cbow_mean\n",
    "        self.hashfxn = hashfxn\n",
    "        self.iter = iter\n",
    "        self.null_word = null_word\n",
    "        self.trim_rule = trim_rule\n",
    "        self.sorted_vocab = sorted_vocab\n",
    "        self.batch_words = batch_words\n",
    "        self.compute_loss = compute_loss\n",
    "        self.callbacks = callbacks\n",
    "        self.max_final_vocab = max_final_vocab\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model_ = Word2Vec(\n",
    "            sentences=X, corpus_file=None,\n",
    "            size=self.size, alpha=self.alpha, window=self.window, min_count=self.min_count,\n",
    "            max_vocab_size=self.max_vocab_size, sample=self.sample, seed=self.seed,\n",
    "            workers=self.workers, min_alpha=self.min_alpha, sg=self.sg, hs=self.hs,\n",
    "            negative=self.negative, ns_exponent=self.ns_exponent, cbow_mean=self.cbow_mean,\n",
    "            hashfxn=self.hashfxn, iter=self.iter, null_word=self.null_word,\n",
    "            trim_rule=self.trim_rule, sorted_vocab=self.sorted_vocab, batch_words=self.batch_words,\n",
    "            compute_loss=self.compute_loss, callbacks=self.callbacks,\n",
    "            max_final_vocab=self.max_final_vocab)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_embeddings = np.array([self._get_embedding(words) for words in X])\n",
    "        return X_embeddings\n",
    "\n",
    "    def _get_embedding(self, words):\n",
    "        valid_words = [word for word in words if word in self.model_.wv.vocab]\n",
    "        if valid_words:\n",
    "            embedding = np.zeros((len(valid_words), self.size), dtype=np.float32)\n",
    "            for idx, word in enumerate(valid_words):\n",
    "                embedding[idx] = self.model_.wv[word]\n",
    "\n",
    "            return np.mean(embedding, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('w2v',\n",
       "                 GensimWord2VecVectorizer(alpha=0.25, min_count=7, sg=1,\n",
       "                                          size=200)),\n",
       "                ('xgb',\n",
       "                 XGBClassifier(base_score=None, booster=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=None, gamma=None, gpu_id=None,\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints=None, learning_rate=0.1,\n",
       "                               max_delta_step=None, max_depth=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=-1, num_parallel_tree=None,\n",
       "                               random_state=None, reg_alpha=None,\n",
       "                               reg_lambda=None, scale_pos_weight=None,\n",
       "                               subsample=None, tree_method=None,\n",
       "                               validate_parameters=None, verbosity=None))])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "gensim_word2vec_tr = GensimWord2VecVectorizer(size=200, min_count=7, sg=1, alpha=0.25, iter=10)\n",
    "xgb = XGBClassifier(learning_rate=0.1, n_estimators=100, n_jobs=-1)\n",
    "w2v_xgb = Pipeline([\n",
    "    ('w2v', gensim_word2vec_tr), \n",
    "    ('xgb', xgb)\n",
    "])\n",
    "w2v_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:19:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('w2v',\n",
       "                 GensimWord2VecVectorizer(alpha=0.25, min_count=7, sg=1,\n",
       "                                          size=200)),\n",
       "                ('xgb',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints='', learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=6,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=-1, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "                               subsample=1, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w2v_xgb.fit(X_train, y_train)\n",
    "\n",
    "w2v_xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy 0.9951830443159922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[141,   0,   0,   0,   0,   0],\n",
       "       [  0,  59,   0,   1,   0,   0],\n",
       "       [  0,   0, 245,   1,   0,   0],\n",
       "       [  0,   0,   0,  87,   0,   0],\n",
       "       [  0,   1,   0,   1, 229,   0],\n",
       "       [  0,   0,   0,   1,   0, 272]], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_train_pred = w2v_xgb.predict(X_train)\n",
    "print('Training set accuracy %s' % accuracy_score(y_train, y_train_pred))\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy 0.4807692307692308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[16,  1,  6,  1,  6,  5],\n",
       "       [ 0,  1,  7,  1,  2,  4],\n",
       "       [ 1,  0, 41,  0, 10,  9],\n",
       "       [ 0,  0,  2,  4,  6, 10],\n",
       "       [ 3,  0,  8,  1, 27, 19],\n",
       "       [ 4,  0, 13,  2, 14, 36]], dtype=int64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_test_pred = w2v_xgb.predict(X_test)\n",
    "print('Test set accuracy %s' % accuracy_score(y_test, y_test_pred))\n",
    "confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('east', 0.4596766233444214),\n",
       " ('it’s', 0.412503719329834),\n",
       " ('plan', 0.3902943432331085),\n",
       " ('never', 0.38893717527389526),\n",
       " ('every', 0.38804322481155396),\n",
       " ('trust', 0.3701404333114624),\n",
       " ('actually', 0.3690882921218872),\n",
       " ('come', 0.36514076590538025),\n",
       " (\"doesn't\", 0.35984739661216736),\n",
       " ('white', 0.3423382639884949)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "vocab_size = len(w2v_xgb.named_steps['w2v'].model_.wv.index2word)\n",
    "print('vocabulary size:', vocab_size)\n",
    "w2v_xgb.named_steps['w2v'].model_.wv.most_similar(positive=['biden'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from  pyspark.ml.feature import CountVectorizerModel\n",
    "from pyspark.ml.feature import  IDFModel \n",
    "import numpy.core.defchararray as np_f\n",
    "\n",
    "    \n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "  \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    df2 = df.withColumn(\"label\", when(df.label == \"#biden\",1)\n",
    "                                 .when(df.label == \"#inflation\",2)\n",
    "                    .when(df.label == \"#china\",3)\n",
    "                    .when(df.label == \"#stopasianhate\",4)\n",
    "                    .when(df.label == \"#covid\",5)\n",
    "                    .when(df.label == \"#vaccine\",6)\n",
    "                                 .when(df.label.isNull() ,\"\")\n",
    "                                 .otherwise(df.label))\n",
    "    df2 = df2.withColumn('tweet_text', regexp_replace('tweet_text', r'[#@][^\\s#@]+', ''))\n",
    "\n",
    "\n",
    "    tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"tweet_text_tokens\")\n",
    "    data_words = tokenizer.transform(df2)\n",
    "    data_words.show()\n",
    "    \n",
    "    DF = data_words.toPandas()\n",
    "    X = DF['tweet_text_tokens']\n",
    "    result = w2v_xgb.predict(X)\n",
    "    result_pd = pd.DataFrame(result)\n",
    "    pred = spark.createDataFrame(result_pd)\n",
    "    result =  pred.selectExpr(\"0 as prediction\")\n",
    "    result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "========= 2021-05-14 08:06:30 =========\n",
      "+----------+-------------------+--------------------+\n",
      "|     label|           tweet_id|          tweet_text|\n",
      "+----------+-------------------+--------------------+\n",
      "|#inflation|1393082618384502784|The next five yea...|\n",
      "+----------+-------------------+--------------------+\n",
      "\n",
      "+-----+-------------------+--------------------+--------------------+\n",
      "|label|           tweet_id|          tweet_text|   tweet_text_tokens|\n",
      "+-----+-------------------+--------------------+--------------------+\n",
      "|    2|1393082618384502784|The next five yea...|[the, next, five,...|\n",
      "+-----+-------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
